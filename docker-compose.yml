services:
  toolbox:
    build:
      context: ./toolbox
      dockerfile: Dockerfile
    container_name: toolbox
    ports:
      - "8000:8000"
    environment:
      - LOG_LEVEL=DEBUG
    networks:
      - mcp-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 3

  llm-gateway:
    build:
      context: ./llm-gateway
      dockerfile: Dockerfile
    container_name: llm-gateway
    ports:
      - "8003:8003"
    environment:
      - HOST=0.0.0.0
      - PORT=8003
      - LOG_LEVEL=INFO
      - CACHE_ENABLED=true
      - CACHE_TTL=3600
      - CACHE_MAX_SIZE=1000
      - METRICS_ENABLED=true
      # AWS Bedrock
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION}
      - BEDROCK_MODEL_ID=${BEDROCK_MODEL_ID}
      # OpenAI
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_DEFAULT_MODEL=${OPENAI_DEFAULT_MODEL:-gpt-4o}
      # Google Gemini
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - GEMINI_DEFAULT_MODEL=${GEMINI_DEFAULT_MODEL:-gemini-1.5-flash}
    networks:
      - mcp-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 10s
      timeout: 5s
      retries: 3

  agent-http:
    build:
      context: ./agents/agent-http
      dockerfile: Dockerfile
    container_name: agent-http
    ports:
      - "8001:8000"
    environment:
      - MCP_SERVER_URL=http://toolbox:8000
      - LLM_GATEWAY_URL=http://llm-gateway:8003
      - DEFAULT_MODEL=bedrock-nova-pro
      - LOG_LEVEL=DEBUG
      - LANGCHAIN_TRACING_V2=${LANGCHAIN_TRACING_V2:-true}
      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY}
      - LANGCHAIN_PROJECT=${LANGCHAIN_PROJECT:-mcp-agent-http}
      - LANGCHAIN_ENDPOINT=${LANGCHAIN_ENDPOINT:-https://api.smith.langchain.com}
    depends_on:
      toolbox:
        condition: service_healthy
      llm-gateway:
        condition: service_healthy
    networks:
      - mcp-network
    restart: unless-stopped

  agent-websocket:
    build:
      context: ./agents/agent-websocket
      dockerfile: Dockerfile
    container_name: agent-websocket
    ports:
      - "8002:8000"
    environment:
      - MCP_SERVER_URL=http://toolbox:8000
      - LLM_GATEWAY_URL=http://llm-gateway:8003
      - DEFAULT_MODEL=bedrock-nova-pro
      - LOG_LEVEL=DEBUG
      - LANGCHAIN_TRACING_V2=${LANGCHAIN_TRACING_V2:-true}
      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY}
      - LANGCHAIN_PROJECT=${LANGCHAIN_PROJECT:-mcp-agent-websocket}
      - LANGCHAIN_ENDPOINT=${LANGCHAIN_ENDPOINT:-https://api.smith.langchain.com}
    depends_on:
      toolbox:
        condition: service_healthy
      llm-gateway:
        condition: service_healthy
    networks:
      - mcp-network
    restart: unless-stopped

networks:
  mcp-network:
    driver: bridge
