# ğŸ¤– LangGraph Multi-Agent System + MCP + LLM Gateway

Sistema multi-agente inteligente con **Model Context Protocol (MCP)**, **LLM Gateway centralizado** y soporte para mÃºltiples proveedores de IA (AWS Bedrock, OpenAI, Google Gemini).

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa una arquitectura de microservicios para agentes de IA que:
- ğŸ§  **LLM Gateway Centralizado**: GestiÃ³n unificada de mÃºltiples proveedores de LLMs
- ğŸ”§ **MCP Toolbox**: Servidor de herramientas usando Model Context Protocol sobre HTTP
- ğŸ¤– **MÃºltiples Agentes**: HTTP REST y WebSocket para diferentes tipos de integraciÃ³n
- ğŸ“Š **LangGraph**: OrquestaciÃ³n avanzada de flujos de trabajo
- ğŸ³ **Containerizado**: Todo en Docker para fÃ¡cil deployment
- â˜ï¸ **Production Ready**: Listo para Kubernetes/AWS EKS

### ğŸ¯ CaracterÃ­sticas Principales

- âœ… **SelecciÃ³n dinÃ¡mica de modelos**: Cambia entre Bedrock, OpenAI y Gemini desde el prompt
- âœ… **Cache inteligente**: Respuestas cacheadas con TTL configurable
- âœ… **MÃ©tricas en tiempo real**: Tracking de costos, tokens y latencia
- âœ… **Manejo de herramientas**: EjecuciÃ³n de tools a travÃ©s de MCP
- âœ… **Streaming**: Soporte WebSocket para respuestas en tiempo real
- âœ… **Health checks**: Monitoreo de salud de todos los servicios

## ğŸ—ï¸ Arquitectura

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Browser/Client    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ WebSocket                   â”‚ HTTP REST
                â–¼                             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Agent WebSocket      â”‚    â”‚  Agent HTTP           â”‚
    â”‚  Port: 8002           â”‚    â”‚  Port: 8001           â”‚
    â”‚  â€¢ Streaming real-timeâ”‚    â”‚  â€¢ REST API           â”‚
    â”‚  â€¢ MÃºltiples clientes â”‚    â”‚  â€¢ Request/Response   â”‚
    â”‚  â€¢ FastAPI + WS       â”‚    â”‚  â€¢ FastAPI            â”‚
    â”‚  â€¢ LangGraph          â”‚    â”‚  â€¢ LangGraph          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                            â”‚
                â”‚      MCP Protocol          â”‚
                â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                â”‚            â”‚               â”‚
                â–¼            â–¼               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚  LLM Gateway     â”‚  â”‚   MCP Toolbox          â”‚
    â”‚  Port: 8003      â”‚  â”‚   Port: 8000           â”‚
    â”‚                  â”‚  â”‚                        â”‚
    â”‚  3 Providers:    â”‚  â”‚   Tools:               â”‚
    â”‚  â€¢ Bedrock Nova  â”‚  â”‚   â€¢ add                â”‚
    â”‚  â€¢ OpenAI GPT-4o â”‚  â”‚   â€¢ multiply           â”‚
    â”‚  â€¢ Gemini Flash  â”‚  â”‚   â€¢ uppercase          â”‚
    â”‚                  â”‚  â”‚   â€¢ count_words        â”‚
    â”‚  Features:       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚  â€¢ Cache (TTL)   â”‚
    â”‚  â€¢ Metrics       â”‚
    â”‚  â€¢ Cost tracking â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                
        Docker Network (mcp-network)
```

## ğŸ”§ Componentes

### 1. ğŸ§  LLM Gateway (Puerto 8003)
**Servidor centralizado de gestiÃ³n de LLMs**

- **PropÃ³sito**: Abstrae y unifica el acceso a mÃºltiples proveedores de IA
- **Proveedores soportados**:
  - AWS Bedrock Nova Pro (`bedrock-nova-pro`)
  - OpenAI GPT-4o (`gpt-4o`)
  - Google Gemini 1.5 Flash (`gemini-pro`)
- **CaracterÃ­sticas**:
  - ğŸ’° **CÃ¡lculo de costos**: Estima costos por request
  - ğŸš€ **Cache TTL**: Reduce llamadas a APIs externas
  - ğŸ“Š **MÃ©tricas**: Requests, tokens, latencia, hit rate
  - ğŸ”Œ **PatrÃ³n Registry**: FÃ¡cil agregar nuevos LLMs
  - ğŸ” **Credenciales centralizadas**: Los agentes no necesitan API keys

**Endpoints**:
- `GET /mcp/llm/list` - Lista modelos disponibles
- `POST /mcp/llm/generate` - Genera respuesta con modelo seleccionado
- `GET /metrics` - Obtiene mÃ©tricas del gateway
- `POST /cache/clear` - Limpia el cache

### 2. ğŸ› ï¸ MCP Toolbox (Puerto 8000)
**Servidor de herramientas con Model Context Protocol**

- **Protocolo**: MCP sobre HTTP REST
- **4 Herramientas**:
  - `add(a, b)` - Suma dos nÃºmeros
  - `multiply(a, b)` - Multiplica dos nÃºmeros
  - `uppercase(text)` - Convierte texto a mayÃºsculas
  - `count_words(text)` - Cuenta palabras en un texto

**Endpoints**:
- `GET /mcp/tools/list` - Lista herramientas disponibles
- `POST /mcp/tools/call` - Ejecuta una herramienta

### 3. ğŸ¤– Agent HTTP (Puerto 8001)
**Agente con API REST**

- **Framework**: FastAPI + LangGraph
- **Tipo**: Request/Response tradicional
- **Uso**: Integraciones sÃ­ncronas, APIs externas
- **CaracterÃ­sticas**:
  - SelecciÃ³n de modelo por request
  - DetecciÃ³n automÃ¡tica de modelo en prompt
  - Tracking de pasos de ejecuciÃ³n

**Endpoint**:
```bash
POST /process
{
  "input": "usa gemini, cuanto es 5 + 3",
  "model": "gemini-pro"  # Opcional
}
```

### 4. ğŸ”Œ Agent WebSocket (Puerto 8002)
**Agente con comunicaciÃ³n en tiempo real**

- **Framework**: FastAPI WebSocket + LangGraph
- **Tipo**: Streaming bidireccional
- **Uso**: Interfaces conversacionales, dashboards
- **CaracterÃ­sticas**:
  - MÃºltiples clientes concurrentes
  - Streaming de pasos de ejecuciÃ³n
  - Notificaciones en tiempo real

**ConexiÃ³n**:
```javascript
ws://localhost:8002/ws/{connection_id}
```

## ğŸ“ Estructura del Proyecto

```
MCP-Example/
â”œâ”€â”€ llm-gateway/                     # ğŸ§  LLM Gateway (NUEVO)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ models/                 # Implementaciones LLM
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py            # Clase abstracta
â”‚   â”‚   â”‚   â”œâ”€â”€ bedrock.py         # AWS Bedrock
â”‚   â”‚   â”‚   â”œâ”€â”€ openai.py          # OpenAI GPT-4
â”‚   â”‚   â”‚   â””â”€â”€ gemini.py          # Google Gemini
â”‚   â”‚   â”œâ”€â”€ cache.py               # Sistema de cache TTL
â”‚   â”‚   â”œâ”€â”€ metrics.py             # MÃ©tricas y tracking
â”‚   â”‚   â”œâ”€â”€ registry.py            # Registry de LLMs
â”‚   â”‚   â”œâ”€â”€ config.py              # ConfiguraciÃ³n
â”‚   â”‚   â””â”€â”€ server.py              # FastAPI server MCP
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ agents/                          # Agentes del sistema
â”‚   â”œâ”€â”€ agent-http/                  # Agent REST API
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ graph/              # LangGraph workflow
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ nodes.py        # Nodos del grafo
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ state.py        # Estado del agente
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ workflow.py     # DefiniciÃ³n del workflow
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_client/         # Cliente LLM Gateway (NUEVO)
â”‚   â”‚   â”‚   â”œâ”€â”€ mcp_client/         # Cliente MCP Toolbox
â”‚   â”‚   â”‚   â”œâ”€â”€ api/                # FastAPI routes
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py           # ConfiguraciÃ³n
â”‚   â”‚   â”‚   â””â”€â”€ main.py             # Entry point
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â”‚
â”‚   â””â”€â”€ agent-websocket/             # Agent WebSocket
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ graph/              # LangGraph workflow
â”‚       â”‚   â”œâ”€â”€ llm_client/         # Cliente LLM Gateway (NUEVO)
â”‚       â”‚   â”œâ”€â”€ mcp_client/         # Cliente MCP Toolbox
â”‚       â”‚   â”œâ”€â”€ websocket/          # WebSocket handlers
â”‚       â”‚   â”œâ”€â”€ config.py           # ConfiguraciÃ³n
â”‚       â”‚   â””â”€â”€ main.py             # Entry point
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ mcp-server/                      # MCP Toolbox Server
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ tools/                  # 4 herramientas
â”‚   â”‚   â”‚   â”œâ”€â”€ calculator.py
â”‚   â”‚   â”‚   â””â”€â”€ text_tools.py
â”‚   â”‚   â”œâ”€â”€ server.py               # MCP server HTTP
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚   â”‚   â”œâ”€â”€ server.py               # MCP server HTTP
â”‚   â”‚   â””â”€â”€ config.py               # ConfiguraciÃ³n
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ k8s/                             # Manifiestos Kubernetes
â”‚   â”œâ”€â”€ namespace.yaml
â”‚   â”œâ”€â”€ mcp-toolbox-*.yaml
â”‚
â”œâ”€â”€ frontend/                        # Interfaz web (opcional)
â”œâ”€â”€ k8s/                             # Manifiestos Kubernetes
â”‚   â”œâ”€â”€ namespace.yaml
â”‚   â”œâ”€â”€ llm-gateway-*.yaml          # Deployment LLM Gateway
â”‚   â”œâ”€â”€ mcp-toolbox-*.yaml
â”‚   â”œâ”€â”€ agent-*.yaml
â”‚   â”œâ”€â”€ websocket-agent-*.yaml
â”‚   â””â”€â”€ ingress.yaml
â”‚
â”œâ”€â”€ docs/                            # DocumentaciÃ³n
â”‚   â”œâ”€â”€ DEPLOYMENT_EKS.md           # GuÃ­a AWS EKS
â”‚   â””â”€â”€ WEBSOCKET_AGENT.md          # Docs WebSocket
â”‚
â”œâ”€â”€ docker-compose.yml               # OrquestaciÃ³n Docker
â”œâ”€â”€ test-websocket.html              # Cliente HTML WebSocket
â”œâ”€â”€ .env                             # Variables de entorno (NO SUBIR)
â””â”€â”€ README.md
```

## ğŸš€ InstalaciÃ³n y Uso

### Prerrequisitos

- Docker y Docker Compose instalados
- **Credenciales de al menos uno de:**
  - AWS (para Bedrock Nova Pro)
  - OpenAI (para GPT-4o)
  - Google Cloud (para Gemini)

### ConfiguraciÃ³n

1. **Clona el repositorio**

```bash
git clone https://github.com/LeonAchata/MCP-Server-Prueba.git
cd MCP-Example
```

2. **Configura las variables de entorno**

Crea el archivo `.env` en la raÃ­z del proyecto:

```bash
# LLM Gateway Configuration
HOST=0.0.0.0
PORT=8003
LOG_LEVEL=INFO

# Cache Configuration
CACHE_ENABLED=true
CACHE_TTL=3600
CACHE_MAX_SIZE=1000

# AWS Bedrock Credentials (Opcional)
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=tu_access_key
AWS_SECRET_ACCESS_KEY=tu_secret_key
BEDROCK_MODEL_ID=us.amazon.nova-pro-v1:0

# OpenAI Credentials (Opcional)
OPENAI_API_KEY=sk-proj-...
OPENAI_DEFAULT_MODEL=gpt-4o

# Google Gemini Credentials (Opcional)
GOOGLE_API_KEY=AIzaSy...
GEMINI_DEFAULT_MODEL=gemini-1.5-flash

# MCP Configuration
MCP_SERVER_URL=http://toolbox:8000
LLM_GATEWAY_URL=http://llm-gateway:8003
```

**âš ï¸ Notas importantes:**
- Configura al menos un proveedor de LLM (Bedrock, OpenAI o Gemini)
- Si usas AWS, asegÃºrate de tener acceso a Bedrock Nova Pro en tu regiÃ³n
- Para OpenAI, necesitas crÃ©ditos en tu cuenta
- Para Gemini, habilita la API en Google Cloud Console

### EjecuciÃ³n

**Construir e iniciar todos los contenedores:**

```bash
docker-compose up --build -d
```

El sistema iniciarÃ¡ 4 servicios:
- ğŸ§  **LLM Gateway** en `http://localhost:8003`
- ğŸ”§ **MCP Toolbox** en `http://localhost:8000` (interno)
- ğŸ“¡ **Agent HTTP** en `http://localhost:8001`
- ğŸ”Œ **Agent WebSocket** en `http://localhost:8002`

**Ver logs en tiempo real:**
```bash
# Todos los servicios
docker-compose logs -f

# Servicio especÃ­fico
docker-compose logs -f llm-gateway
docker-compose logs -f agent-http
```

**Verificar estado de los servicios:**
```bash
docker-compose ps
```

**Detener el sistema:**
```bash
docker-compose down
```

**Reconstruir un servicio especÃ­fico:**
```bash
docker-compose build llm-gateway
docker-compose up -d llm-gateway
```

## ğŸ“¡ API Reference

### ğŸ§  LLM Gateway (Port 8003)

#### GET /health
Verifica el estado del gateway:
```bash
curl http://localhost:8003/health
```

#### GET /mcp/llm/list
Lista todos los modelos disponibles:
```bash
curl -X GET http://localhost:8003/mcp/llm/list
```

Respuesta:
```json
{
  "llms": [
    {
      "name": "bedrock-nova-pro",
      "provider": "aws",
      "description": "AWS Bedrock Nova Pro - Advanced reasoning model"
    },
    {
      "name": "gpt-4o",
      "provider": "openai",
      "description": "OpenAI GPT-4o - Most capable model"
    },
    {
      "name": "gemini-pro",
      "provider": "google",
      "description": "Google Gemini - Advanced multimodal AI model (using gemini-1.5-flash)"
    }
  ]
}
```

#### POST /mcp/llm/generate
Genera una respuesta con el modelo especificado:
```bash
curl -X POST http://localhost:8003/mcp/llm/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-pro",
    "messages": [
      {"role": "user", "content": "Explain quantum computing"}
    ],
    "temperature": 0.7,
    "max_tokens": 2000
  }'
```

#### GET /metrics
Obtiene mÃ©tricas del gateway:
```bash
curl http://localhost:8003/metrics
```

Respuesta:
```json
{
  "total_requests": 42,
  "total_tokens": 15234,
  "total_cost_usd": 0.0523,
  "average_latency_ms": 1234.5,
  "cache_hit_rate": 0.35,
  "requests_by_model": {
    "bedrock-nova-pro": 20,
    "gpt-4o": 12,
    "gemini-pro": 10
  }
}
```

#### POST /cache/clear
Limpia el cache:
```bash
curl -X POST http://localhost:8003/cache/clear
```

### ğŸ”§ MCP Toolbox (Port 8000)

#### GET /health
```bash
curl http://localhost:8000/health
```

Respuesta:
```json
{
  "status": "healthy",
  "service": "mcp-toolbox",
  "tools_count": 4,
  "protocol": "MCP over HTTP REST"
}
```

#### POST /mcp/tools/list
Lista todas las herramientas disponibles:
```bash
curl -X POST http://localhost:8000/mcp/tools/list
```

#### POST /mcp/tools/call
Ejecuta una herramienta:
```bash
curl -X POST http://localhost:8000/mcp/tools/call \
  -H "Content-Type: application/json" \
  -d '{"name": "add", "arguments": {"a": 5, "b": 3}}'
```

### ğŸ¤– Agent HTTP - REST API (Port 8001)

#### GET /health
Verifica el estado del agente:

```bash
curl http://localhost:8001/health
```

Respuesta:
```json
{
  "status": "healthy",
  "mcp_connected": true,
  "bedrock_available": true
}
```

#### POST /process
Procesa una query usando el agente con LangGraph.

**Sintaxis bÃ¡sica:**
```bash
curl -X POST http://localhost:8001/process \
  -H "Content-Type: application/json" \
  -d '{
    "input": "Â¿CuÃ¡nto es 5 + 3?",
    "model": "bedrock-nova-pro"
  }'
```

**Ejemplo 1: Suma con Bedrock (default)**
```bash
curl -X POST http://localhost:8001/process \
  -H "Content-Type: application/json" \
  -d '{"input": "Â¿CuÃ¡nto es 5 + 3?"}'
```

**Ejemplo 2: Con Gemini (especificado)**
```bash
curl -X POST http://localhost:8001/process \
  -H "Content-Type: application/json" \
  -d '{"input": "Multiplica 7 por 8", "model": "gemini-pro"}'
```

**Ejemplo 3: DetecciÃ³n automÃ¡tica de modelo**
```bash
curl -X POST http://localhost:8001/process \
  -H "Content-Type: application/json" \
  -d '{"input": "usa gemini, convierte HELLO a mayÃºsculas"}'
```

**Ejemplo 4: Operaciones complejas**
```bash
curl -X POST http://localhost:8001/process \
  -H "Content-Type: application/json" \
  -d '{"input": "Multiplica 25 por 8, luego convierte el resultado a texto en mayÃºsculas"}'
```

**Con PowerShell:**
```powershell
$body = @{
    input = "usa gemini, cuanto es 10 + 5"
} | ConvertTo-Json

Invoke-WebRequest -Uri "http://localhost:8001/process" `
  -Method POST `
  -Body $body `
  -ContentType "application/json"
```

Respuesta:
```json
{
  "result": "La suma de 5 y 3 es 8",
  "steps": [
    {
      "node": "process_input",
      "timestamp": "2024-11-03T19:00:00",
      "input": "Â¿CuÃ¡nto es 5 + 3?",
      "model_selected": "bedrock-nova-pro"
    },
    {
      "node": "llm",
      "timestamp": "2024-11-03T19:00:01",
      "model": "bedrock-nova-pro",
      "has_tool_calls": true
    },
    {
      "node": "tool_execution",
      "timestamp": "2024-11-03T19:00:01",
      "tools": [
        {"name": "add", "args": {"a": 5, "b": 3}, "result": "8"}
      ]
    },
    {
      "node": "llm",
      "timestamp": "2024-11-03T19:00:02",
      "model": "bedrock-nova-pro",
      "has_tool_calls": false
    },
    {"node": "final_answer", "timestamp": "2024-11-03T19:00:02"}
  ]
}
```

**Modelos disponibles:**
- `bedrock-nova-pro` - AWS Bedrock Nova Pro (default)
- `gpt-4o` - OpenAI GPT-4o
- `gemini-pro` - Google Gemini 1.5 Flash

**DetecciÃ³n automÃ¡tica:**
El agente puede detectar el modelo desde el prompt con palabras clave:
- "usa openai", "use gpt", "con gpt-4" â†’ OpenAI
- "usa gemini", "use google", "con gemini" â†’ Gemini
- "usa bedrock", "use nova", "con aws" â†’ Bedrock

---

### ğŸ”Œ Agent WebSocket - Real-time Streaming (Port 8002)

#### GET /health
Verifica el estado del agente WebSocket:

```bash
curl http://localhost:8002/health
```

Respuesta:
```json
{
  "status": "healthy",
  "service": "websocket-agent",
  "mcp_connected": true,
  "mcp_tools": 4,
  "active_connections": 0
}
```

#### WebSocket /ws/{connection_id}
ConexiÃ³n WebSocket para comunicaciÃ³n en tiempo real con streaming de respuestas.

**Usando el cliente HTML:**
1. Abre `test-websocket.html` en tu navegador
2. La conexiÃ³n se establece automÃ¡ticamente
3. Escribe mensajes como:
   - "Suma 10 y 5"
   - "usa gemini, multiplica 25 por 8"
   - "Convierte HOLA a mayÃºsculas"

**Mensaje con modelo especÃ­fico:**
```javascript
{
  "type": "message",
  "content": "Suma 100 y 50",
  "model": "gemini-pro"  // Opcional
}
```

**Usando JavaScript:**
```javascript
const connectionId = 'user-' + Date.now();
const ws = new WebSocket(`ws://localhost:8002/ws/${connectionId}`);

ws.onopen = () => {
    console.log('Conectado');
    
    // Enviar mensaje con modelo especÃ­fico
    ws.send(JSON.stringify({
        type: 'message',
        content: 'usa gemini, suma 100 y 50',
        model: 'gemini-pro'  // Opcional, tambiÃ©n detecta del texto
    }));
};

ws.onmessage = (event) => {
    const data = JSON.parse(event.data);
    console.log('Recibido:', data);
    
    switch(data.type) {
        case 'connected':
            console.log('âœ… Conectado:', data.message);
            break;
        case 'start':
            console.log('ğŸš€', data.message);
            break;
        case 'step':
            console.log(`âš™ï¸ ${data.node}:`, data.message);
            if (data.model) {
                console.log('  ğŸ§  Modelo:', data.model);
            }
            break;
        case 'tool_call':
            console.log('ğŸ”§ Llamando:', data.tool, data.args);
            break;
        case 'tool_result':
            console.log('âœ… Resultado:', data.tool, 'â†’', data.result);
            break;
        case 'response':
            console.log('ğŸ¤– Respuesta:', data.content);
            break;
        case 'complete':
            console.log('âœ“ Completado en', data.steps, 'pasos');
            break;
        case 'error':
            console.error('âŒ Error:', data.message);
            break;
    }
};

ws.onerror = (error) => console.error('Error:', error);
ws.onclose = () => console.log('Desconectado');
```

**Usando wscat (Node.js):**
```bash
npm install -g wscat
wscat -c ws://localhost:8002/ws/test-client

# Enviar mensaje
> {"type":"message","content":"usa gemini, suma 10 y 5"}

# RecibirÃ¡s streaming en tiempo real:
< {"type":"start","message":"Procesando..."}
< {"type":"step","node":"process_input","model":"gemini-pro"}
< {"type":"step","node":"llm","model":"gemini-pro","message":"Consultando LLM..."}
< {"type":"tool_call","tool":"add","args":{"a":10,"b":5}}
< {"type":"tool_result","tool":"add","result":"15"}
< {"type":"response","content":"La suma de 10 y 5 es 15"}
< {"type":"complete","steps":5}
```
```

**Usando Python:**
```python
import asyncio
import websockets
import json

async def test_websocket():
    uri = "ws://localhost:8002/ws"
    async with websockets.connect(uri) as websocket:
        # Enviar mensaje
        await websocket.send(json.dumps({
            "type": "message",
            "content": "Suma 10 y 5"
        }))
        
        # Recibir respuestas en streaming
        while True:
            response = await websocket.recv()
            data = json.loads(response)
            print(f"{data['type']}: {data}")
            
            if data['type'] == 'complete':
                break

asyncio.run(test_websocket())
```

## ğŸ› ï¸ Herramientas Disponibles

El MCP Server expone 4 herramientas que Claude puede usar:

| Herramienta | DescripciÃ³n | ParÃ¡metros |
|-------------|-------------|------------|
| `add` | Suma dos nÃºmeros | `a: float, b: float` |
| `multiply` | Multiplica dos nÃºmeros | `a: float, b: float` |
| `uppercase` | Convierte texto a mayÃºsculas | `text: string` |
| `count_words` | Cuenta palabras en un texto | `text: string` |

## ğŸ’¡ Ejemplos de Uso Completos

### ğŸ§  SelecciÃ³n de Modelos LLM

**Modelo por defecto (Bedrock):**
```bash
curl -X POST http://localhost:8001/process \
  -H "Content-Type: application/json" \
  -d '{"input": "Suma 10 y 5"}'
```

**Especificando modelo explÃ­citamente:**
```bash
# Con Gemini
curl -X POST http://localhost:8001/process \
  -H "Content-Type: application/json" \
  -d '{"input": "Multiplica 7 por 8", "model": "gemini-pro"}'

# Con OpenAI (si tienes crÃ©ditos)
curl -X POST http://localhost:8001/process \
  -H "Content-Type: application/json" \
  -d '{"input": "Cuenta las palabras en: hola mundo", "model": "gpt-4o"}'
```

**DetecciÃ³n automÃ¡tica desde el prompt:**
```bash
# Detecta Gemini
curl -X POST http://localhost:8001/process \
  -H "Content-Type: application/json" \
  -d '{"input": "usa gemini, cuanto es 15 + 25"}'

# Detecta OpenAI
curl -X POST http://localhost:8001/process \
  -H "Content-Type: application/json" \
  -d '{"input": "con gpt-4, convierte HELLO a mayÃºsculas"}'

# Detecta Bedrock
curl -X POST http://localhost:8001/process \
  -H "Content-Type: application/json" \
  -d '{"input": "usa bedrock, multiplica 3 por 9"}'
```

### ğŸ“¡ HTTP REST Agent

**MatemÃ¡ticas bÃ¡sicas:**
```bash
curl -X POST http://localhost:8001/process \
  -H "Content-Type: application/json" \
  -d '{"input": "Calcula 10 multiplicado por 5"}'
```

**Procesamiento de texto:**
```bash
curl -X POST http://localhost:8001/process \
  -H "Content-Type: application/json" \
  -d '{"input": "Convierte hello world a mayÃºsculas"}'
```

**CombinaciÃ³n de herramientas:**
```bash
curl -X POST http://localhost:8001/process \
  -H "Content-Type: application/json" \
  -d '{"input": "Suma 4 y 6, luego multiplica el resultado por 2"}'
```

**Con PowerShell:**
```powershell
# Suma con Bedrock
$body = '{"input":"Suma 100 y 50"}'
Invoke-WebRequest -Uri "http://localhost:8001/process" -Method POST -Body $body -ContentType "application/json"

# MultiplicaciÃ³n con Gemini
$body = '{"input":"usa gemini, multiplica 25 por 8"}'
Invoke-WebRequest -Uri "http://localhost:8001/process" -Method POST -Body $body -ContentType "application/json"

# Texto
$body = '{"input":"Convierte HOLA MUNDO a mayÃºsculas y cuenta las palabras"}'
Invoke-WebRequest -Uri "http://localhost:8001/process" -Method POST -Body $body -ContentType "application/json"
```

### ğŸ”Œ WebSocket Agent

**Usando el cliente HTML (Recomendado):**
1. Abre el archivo `test-websocket.html` en tu navegador
2. VerÃ¡s una interfaz bonita con el estado de conexiÃ³n
3. Escribe en el input y presiona Enter o clic en "Enviar"
4. Observa el streaming en tiempo real de cada paso
5. Los steps mostrarÃ¡n el modelo usado (en el campo `model`)

**Ejemplos de mensajes:**
- "Suma 10 y 5"
- "usa gemini, multiplica 7 por 8"
- "con gpt-4, convierte HELLO a mayÃºsculas"
- "usa bedrock, cuenta palabras en: el cielo es azul"

**Pruebas desde lÃ­nea de comandos:**
```bash
# Instalar wscat
npm install -g wscat

# Conectar
wscat -c ws://localhost:8002/ws/test-123

# Probar diferentes comandos:
> {"type":"message","content":"Suma 10 y 5"}
> {"type":"message","content":"usa gemini, multiplica 100 por 2"}
> {"type":"message","content":"Convierte python a mayÃºsculas","model":"gemini-pro"}
> {"type":"message","content":"Cuenta las palabras en: El MCP es genial"}
```

### ğŸ§ª Verificar MÃ©tricas del LLM Gateway

```bash
# Ver mÃ©tricas actuales
curl http://localhost:8003/metrics

# Limpiar cache
curl -X POST http://localhost:8003/cache/clear

# Listar modelos disponibles
curl http://localhost:8003/mcp/llm/list
```

## ğŸ” Logs y Debugging

**Ver todos los logs en tiempo real:**
```bash
docker-compose logs -f
```

**Ver logs de un servicio especÃ­fico:**
```bash
docker-compose logs -f llm-gateway
docker-compose logs -f agent-http
docker-compose logs -f agent-websocket
docker-compose logs -f toolbox
```

**Ver Ãºltimas 50 lÃ­neas:**
```bash
docker-compose logs --tail 50 agent-http
```

**Buscar errores en PowerShell:**
```powershell
docker-compose logs agent-http | Select-String -Pattern "error|Error|ERROR"
```

**Los logs muestran:**
- âœ… InicializaciÃ³n del LLM Gateway con 3 proveedores
- âœ… ConexiÃ³n MCP client â†” servers
- âœ… Discovery de herramientas (4 tools)
- âœ… SelecciÃ³n de modelo (Bedrock/OpenAI/Gemini)
- âœ… Llamadas a LLMs con cache hit/miss
- âœ… EjecuciÃ³n de herramientas via MCP
- âœ… MÃ©tricas de costos y tokens
- âœ… Conexiones WebSocket activas
- âœ… Streaming de mensajes en tiempo real

## ğŸ›‘ Detener el Sistema

```bash
docker-compose down
```

## ğŸ”§ Desarrollo

### Reconstruir despuÃ©s de cambios

```bash
docker-compose up --build
```

### Ver logs de un servicio especÃ­fico

```bash
docker-compose logs -f agent
docker-compose logs -f mcp-server
```

## ğŸ“š TecnologÃ­as

- **Python 3.11** - Runtime
- **FastAPI** - Framework web para REST y WebSocket
- **LangGraph** - OrquestaciÃ³n de workflows con grafos
- **LangChain** - Framework para LLM
- **Amazon Bedrock** - Nova Pro (modelo LLM)
- **MCP (Model Context Protocol)** - Protocolo de herramientas sobre HTTP REST
- **WebSocket** - ComunicaciÃ³n bidireccional en tiempo real
- **Docker & Docker Compose** - ContainerizaciÃ³n y orquestaciÃ³n
- **httpx** - Cliente HTTP asÃ­ncrono
- **boto3** - SDK de AWS para Bedrock

## âš ï¸ Notas Importantes

- **NO subir el archivo `.env`** a GitHub (ya estÃ¡ en `.gitignore`)
- Las credenciales de AWS son sensibles - manÃ©jalas con cuidado
## ğŸ“ Notas Importantes

- **Arquitectura de microservicios**: 4 contenedores independientes (LLM Gateway, Toolbox, Agent HTTP, Agent WebSocket)
- **LLM Gateway centralizado**: Un solo punto para gestionar mÃºltiples proveedores de IA
- **Credenciales seguras**: Solo el LLM Gateway tiene las API keys, los agentes no las necesitan
- **Cache inteligente**: Reduce costos y mejora latencia con TTL configurable
- **MCP sobre HTTP REST**: Protocolo MCP real con transporte HTTP para compatibilidad K8s
- **SelecciÃ³n dinÃ¡mica de modelos**: Cambia entre Bedrock/OpenAI/Gemini por request o desde el prompt
- **MÃ©tricas en tiempo real**: Tracking de costos, tokens, latencia y cache hit rate
- **Listo para Kubernetes**: Funciona perfecto en EKS con service discovery
- **WebSocket vs HTTP**: WebSocket para UIs interactivas, HTTP para integraciones
- **Arquitectura centralizada**: Ambos agentes comparten el mismo Toolbox y LLM Gateway
- Los contenedores se reinician automÃ¡ticamente si fallan
- Si tu `AWS_SECRET_ACCESS_KEY` tiene `/`, regenera las credenciales (causa errores de firma)

## ğŸ¯ Casos de Uso

### CuÃ¡ndo usar Agent HTTP (REST):
- âœ… Integraciones con otros servicios/APIs
- âœ… APIs pÃºblicas REST
- âœ… Webhooks
- âœ… Automatizaciones batch
- âœ… Sistemas que necesitan caching
- âœ… Request/response simple

### CuÃ¡ndo usar Agent WebSocket:
- âœ… Chatbots interactivos
- âœ… Aplicaciones de chat en tiempo real
- âœ… Dashboards que necesitan updates live
- âœ… Streaming de respuestas largas
- âœ… Notificaciones push
- âœ… Ver el "pensamiento" del agente paso a paso

### CuÃ¡ndo usar cada LLM:
- **Bedrock Nova Pro** (`bedrock-nova-pro`):
  - âœ… Razonamiento complejo
  - âœ… Largo contexto (300K tokens)
  - âœ… Costo medio
  - âœ… Mejor para anÃ¡lisis profundo

- **OpenAI GPT-4o** (`gpt-4o`):
  - âœ… MÃ¡s capaz y versÃ¡til
  - âœ… Mejor en seguir instrucciones
  - âœ… Costo mÃ¡s alto
  - âœ… Requiere crÃ©ditos activos

- **Gemini 1.5 Flash** (`gemini-pro`):
  - âœ… MÃ¡s rÃ¡pido
  - âœ… Costo mÃ¡s bajo
  - âœ… Bueno para tareas simples
  - âœ… Excelente para producciÃ³n

## ğŸ¢ Deployment a AWS/EKS

Este proyecto estÃ¡ **listo para producciÃ³n** en AWS EKS. Ver guÃ­a completa en [`docs/DEPLOYMENT_EKS.md`](./docs/DEPLOYMENT_EKS.md)

**Resumen de deployment:**

1. **Crear repositorios ECR** para las 4 imÃ¡genes (llm-gateway, toolbox, agent-http, agent-websocket)
2. **Push imÃ¡genes Docker** a ECR
3. **Crear cluster EKS** (o usar existente)
4. **Configurar Secrets Manager** con credenciales (AWS, OpenAI, Gemini)
5. **Aplicar manifiestos K8s**:
   ```bash
   kubectl apply -f k8s/namespace.yaml
   kubectl apply -f k8s/llm-gateway-deployment.yaml
   kubectl apply -f k8s/llm-gateway-service.yaml
   kubectl apply -f k8s/mcp-toolbox-deployment.yaml
   kubectl apply -f k8s/mcp-toolbox-service.yaml
   kubectl apply -f k8s/agent-deployment.yaml
   kubectl apply -f k8s/agent-service.yaml
   kubectl apply -f k8s/websocket-agent-deployment.yaml
   kubectl apply -f k8s/websocket-agent-service.yaml
   kubectl apply -f k8s/ingress.yaml
   ```

**Service Discovery en Kubernetes:**
```yaml
# Los agents se conectan via DNS interno:
LLM_GATEWAY_URL: "http://llm-gateway.mcp-system.svc.cluster.local:8003"
MCP_SERVER_URL: "http://mcp-toolbox.mcp-system.svc.cluster.local:8000"
```

**Arquitectura en EKS:**
```
Internet â†’ ALB Ingress â†’ {
    /api/http â†’ Agent HTTP Service â†’ Agent HTTP Pods
    /api/ws   â†’ WebSocket Agent Service â†’ WebSocket Agent Pods
}

Agent HTTP Pods â”€â”€â”€â”€â”¬â”€â”€â†’ LLM Gateway Service â†’ LLM Gateway Pods â†’ {Bedrock, OpenAI, Gemini}
                    â”‚
WebSocket Agent â”€â”€â”€â”€â”¤
                    â”‚
                    â””â”€â”€â†’ MCP Toolbox Service â†’ MCP Toolbox Pods
```

## ğŸ“– DocumentaciÃ³n Adicional

- [`docs/DEPLOYMENT_EKS.md`](./docs/DEPLOYMENT_EKS.md) - GuÃ­a completa de despliegue en AWS EKS
- [`docs/WEBSOCKET_AGENT.md`](./docs/WEBSOCKET_AGENT.md) - DocumentaciÃ³n del Agent WebSocket
- [`test-websocket.html`](./test-websocket.html) - Cliente de prueba interactivo
- [`k8s/`](./k8s/) - Manifiestos de Kubernetes listos para usar

## ğŸš€ Quick Start

```bash
# 1. Clonar repo
git clone https://github.com/LeonAchata/MCP-Server-Prueba.git
cd MCP-Example

# 2. Configurar credenciales (al menos un proveedor)
nano .env
# Agregar credenciales de AWS Bedrock, OpenAI o Google Gemini

# 3. Levantar servicios
docker-compose up -d

# 4. Verificar que todo estÃ© funcionando
docker-compose ps
docker-compose logs -f

# 5. Probar HTTP Agent
curl -X POST http://localhost:8001/process \
  -H "Content-Type: application/json" \
  -d '{"input":"Suma 10 y 5"}'

# 6. Probar con diferentes modelos
curl -X POST http://localhost:8001/process \
  -H "Content-Type: application/json" \
  -d '{"input":"usa gemini, multiplica 7 por 8"}'

# 7. Probar WebSocket Agent
# Abre test-websocket.html en tu navegador

# 8. Ver mÃ©tricas del gateway
curl http://localhost:8003/metrics
```

## ğŸ”§ Troubleshooting

### Error: "LLM Gateway error (404): LLM 'xxx' not found"
- Verifica que el nombre del modelo sea correcto: `bedrock-nova-pro`, `gpt-4o`, o `gemini-pro`
- Revisa los logs: `docker-compose logs llm-gateway --tail=50`

### Error: OpenAI "insufficient_quota"
- No tienes crÃ©ditos en tu cuenta de OpenAI
- SoluciÃ³n: Usa Bedrock o Gemini, o agrega crÃ©ditos en OpenAI

### Error: Gemini "model not found"
- Verifica que `GEMINI_DEFAULT_MODEL=gemini-1.5-flash` en tu `.env`
- AsegÃºrate de tener habilitada la API de Gemini en Google Cloud

### Error: "RuntimeError: Event loop is closed"
- Ya fue corregido en la versiÃ³n actual
- Si persiste, verifica que estÃ©s usando `async/await` correctamente

### Los contenedores no inician
```bash
# Ver logs detallados
docker-compose logs

# Reconstruir todo desde cero
docker-compose down
docker-compose build --no-cache
docker-compose up -d
```

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas! Si encuentras un bug o tienes una mejora:

1. Fork el repositorio
2. Crea una rama (`git checkout -b feature/amazing-feature`)
3. Commit tus cambios (`git commit -m 'Add amazing feature'`)
4. Push a la rama (`git push origin feature/amazing-feature`)
5. Abre un Pull Request

## ğŸ“ Licencia

Este es un proyecto de aprendizaje personal. Libre de usar para propÃ³sitos educativos.

## ğŸ‘¨â€ğŸ’» Autor

**Leon Achata**
- GitHub: [@LeonAchata](https://github.com/LeonAchata)
- Proyecto: [MCP-Server-Prueba](https://github.com/LeonAchata/MCP-Server-Prueba)

---

**Happy coding! ğŸš€**

*Sistema Multi-Agent con MCP Protocol + LLM Gateway - Production Ready*
